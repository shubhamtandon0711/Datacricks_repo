# ğŸ“¦ Supplier Data Analysis & Visualization in Databricks

Welcome to the **Supplier Data Analysis** repository! This project helps automate the process of collecting, cleaning, analyzing, and visualizing stock-related data from suppliers â€” all within the **Databricks** ecosystem.

---

## ğŸ“ Project Overview

This project aims to streamline:
- ğŸ“¨ Downloading data from suppliers (via SharePoint)
- ğŸ§¹ Preprocessing using Excel macros
- ğŸš€ Uploading cleaned data to Databricks
- ğŸ“Š Analyzing stock coverage and visualizing trends
- ğŸ“¦ Making stock decisions based on QC/Recommended levels

---

## ğŸ§© Folder Structure

```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/                  # PySpark and analysis scripts
â”œâ”€â”€ macros/                   # Excel macro files
â”œâ”€â”€ data/                     # Cleaned data (CSV)
â”œâ”€â”€ outputs/                  # Visualizations or processed output
```

---

## ğŸ” Workflow

### 1ï¸âƒ£ Get Supplier Data

- Download the **March Weidmann** Excel from:
  `SharePoint > Supplier Coordination 06`

- Open the Excel and:
  - Delete old data and other sheets.
  - Paste the new data provided by supplier.
  - Run the macro `macro_weidmann_1`.

- Rename the second sheet to `Sheet1`.

---

### 2ï¸âƒ£ Prepare for Databricks

- Open another Excel file: `Weidmann Data for Databricks`.
- Run the macro (button click).
- Data gets cleaned and reshaped.
- Copy the data and save it as a **new file**:
  `April_2025.xlsx` (Save locally & on SharePoint under supplier data folder)

---

### 3ï¸âƒ£ Upload to Databricks

You can add data to Databricks in different ways:
- ğŸ“¤ Upload via **Data > Add Data** (GUI)
- ğŸ“‚ Use DBFS: `dbfs:/FileStore/filename.csv`
- ğŸ§¼ Programmatically with `dbutils.fs.cp` or API

---

### 4ï¸âƒ£ Load & Process in Databricks

```python
file_path = "dbfs:/FileStore/april_trial_data.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)
```

- Clean columns:
```python
from pyspark.sql.functions import col, round

df = df.withColumn("average_consumption_per_day", 
                   round(col("Recommended Stock level at Supplier") / 66, 2))

df = df.withColumn("stock_coverage_in_weeks_for_supplier", 
                   round(col("QC released stock") / col("average_consumption_per_day") / 5, 2))
```

---

### 5ï¸âƒ£ Analyze & Visualize

- Filter for top 20 parts with lowest average stock over 6 months
- Plot using matplotlib and seaborn:

```python
import matplotlib.pyplot as plt
import seaborn as sns

```

---

## ğŸ› ï¸ Technologies Used

- ğŸ’¾ Microsoft Excel (Macros for preprocessing, Power query to accumulate the data)
- ğŸ§  Databricks (Apache Spark engine)
- ğŸ PySpark (data manipulation)
- ğŸ“ˆ Matplotlib & Seaborn (visualization)

---

## ğŸ™‹â€â™‚ï¸ Author

**Shubham Tandon**  
Werkstudent at QIAGEN

---

## ğŸ“¬ Get in Touch

ğŸ“§ Email: shubhamtandon777@gmail.com



